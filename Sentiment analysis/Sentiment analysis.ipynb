{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сентимент-анализ отзывов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом проекте мы попробуем разобраться с задачей анализа тональности отзывов на примере сентимент-анализа отзывов на товары.\n",
    "Мы будем использовать данные из соревнования Kaggle Inclass https://www.kaggle.com/c/simplesentiment \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим тесты для обучения и теста и посмотрим, что они из себя представляют"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"products_sentiment_train.tsv\", sep='\\t', header=None)\n",
    "test = pd.read_csv(\"products_sentiment_test.tsv\", sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение: (2000, 2)\n",
      "Тест: (500, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Обучение:', df.shape)\n",
    "print('Тест:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 . take around 10,000 640x480 pictures .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i downloaded a trial version of computer assoc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the wrt54g plus the hga7t is a perfect solutio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i dont especially like how music files are uns...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was using the cheapie pail ... and it worked...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0          2 . take around 10,000 640x480 pictures .  1\n",
       "1  i downloaded a trial version of computer assoc...  1\n",
       "2  the wrt54g plus the hga7t is a perfect solutio...  1\n",
       "3  i dont especially like how music files are uns...  0\n",
       "4  i was using the cheapie pail ... and it worked...  1"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1274"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1274\n",
       "0     726\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что обучающая выборка более-менее сбалансирована.                                 \n",
    "Удалим из наших текстов лишние символы и преобразуем с помощью лемматизации и стемминга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem=WordNetLemmatizer()\n",
    "ps =PorterStemmer()\n",
    "lem_reviews = np.array(df.iloc[:,0].values)\n",
    "\n",
    "for i,text in enumerate(df.iloc[:,0].values):\n",
    "    text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)  \n",
    "    words_lem=[lem.lemmatize(word) for word in text.split(' ')]\n",
    "    words_lem_ps = [ps.stem(word) for word in words_lem]   \n",
    "    sentiment =  ' '.join(words_lem_ps)\n",
    "    lem_reviews[i] = sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем несколько вариантов извлечения признаков из текстов и разные линейные обучающие модели "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer и Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем несколько вариантов моделей с CountVectorizer и Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgd_count:  0.75\n",
      "sgd_tfidf:  0.7525000000000001\n"
     ]
    }
   ],
   "source": [
    "print('sgd_count: ',cross_val_score(Pipeline([(\"vectorizer\", CountVectorizer(min_df=5, max_df=2000, ngram_range=(1,3))),(\"classifier\", SGDClassifier())]), \n",
    "                lem_reviews, labels, scoring = 'accuracy').mean())\n",
    "print('sgd_tfidf: ',cross_val_score(Pipeline([(\"vectorizer\", TfidfVectorizer(min_df=5, max_df=2000, ngram_range=(1,3))),(\"classifier\", SGDClassifier())]), \n",
    "                                lem_reviews, labels).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc_count:  0.7424999999999999\n",
      "svc_tfidf:  0.772\n"
     ]
    }
   ],
   "source": [
    "print('svc_count: ',cross_val_score(Pipeline([(\"vectorizer\", CountVectorizer(min_df=5, max_df=2000, ngram_range=(1,3))),(\"classifier\", LinearSVC())]), \n",
    "                lem_reviews, labels, scoring = 'accuracy').mean())\n",
    "print('svc_tfidf: ',cross_val_score(Pipeline([(\"vectorizer\", TfidfVectorizer(min_df=5, max_df=2000, ngram_range=(1,3))),(\"classifier\", LinearSVC())]), \n",
    "                                lem_reviews, labels).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit_count:  0.767\n",
      "logit_tfidf:  0.7629999999999999\n"
     ]
    }
   ],
   "source": [
    "print('logit_count: ',cross_val_score(Pipeline([(\"vectorizer\", CountVectorizer(min_df=5, max_df=2000, ngram_range=(1,3))),(\"classifier\", LogisticRegression())]), \n",
    "                lem_reviews, labels, scoring = 'accuracy').mean())\n",
    "print('logit_tfidf: ',cross_val_score(Pipeline([(\"vectorizer\", TfidfVectorizer(min_df=5, max_df=2000, ngram_range=(1,3))),(\"classifier\", LogisticRegression())]), \n",
    "                                lem_reviews, labels, scoring = 'accuracy').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно что tf-idf работает лучше со всеми моделями. В результате нескольких экспериментов получим следующий набор параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit_tfidf:  0.725\n",
      "svc_tfidf:  0.7875\n",
      "sgd_tfidf:  0.7889999999999999\n"
     ]
    }
   ],
   "source": [
    "print('logit_tfidf: ',cross_val_score(Pipeline([(\"vectorizer\", TfidfVectorizer(max_df=1000, ngram_range=(1,3))),(\"classifier\", LogisticRegression())]), \n",
    "                                lem_reviews, labels, scoring = 'accuracy').mean())\n",
    "print('svc_tfidf: ',cross_val_score(Pipeline([(\"vectorizer\", TfidfVectorizer(max_df=1000, ngram_range=(1,3))),(\"classifier\", LinearSVC())]), \n",
    "                                lem_reviews, labels, scoring = 'accuracy').mean())\n",
    "print('sgd_tfidf: ',cross_val_score(Pipeline([(\"vectorizer\", TfidfVectorizer(max_df=1000, ngram_range=(1,3))),(\"classifier\", SGDClassifier())]), \n",
    "                                lem_reviews, labels, scoring = 'accuracy').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Запишем лучший вариант обработки данных\n",
    "tfv = TfidfVectorizer(max_df=1000, ngram_range=(1,3))\n",
    "tf_idf = tfv.fit_transform(lem_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем подобрать лучшие параметры для наших обучающих моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LinearSVC(random_state=42), param_grid={'C': [2, 3, 1]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = {'penalty':['l1', 'l2'], 'loss':['hinge','squared_hinge'], 'C':[1,2,3]}\n",
    "clf = GridSearchCV(LinearSVC(random_state = 42), param, scoring = 'accuracy')\n",
    "clf.fit(tf_idf, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc_tfidf:  0.7859999999999999\n"
     ]
    }
   ],
   "source": [
    "print('svc_tfidf: ',cross_val_score(LinearSVC(C=2, loss='hinge',random_state = 42), tf_idf, labels, \n",
    "                                    scoring = 'accuracy').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SGDClassifier(random_state=42),\n",
       "             param_grid={'alpha': [0.0001, 0.001, 0.01],\n",
       "                         'loss': ['hinge', 'log', 'modified_huber',\n",
       "                                  'squared_hinge', 'perceptron'],\n",
       "                         'n_jobs': [-1],\n",
       "                         'penalty': ['l1', 'l2', 'elasticnet']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = {'penalty':['l1', 'l2','elasticnet'], 'loss':['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'], \n",
    "         'alpha':[0.0001,0.001,0.01], 'n_jobs': [-1]}\n",
    "clf = GridSearchCV(estimator = SGDClassifier(random_state = 42), param_grid = param, scoring = 'accuracy')\n",
    "clf.fit(tf_idf, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgd_tfidf:  0.7875\n"
     ]
    }
   ],
   "source": [
    "print('sgd_tfidf: ',cross_val_score(SGDClassifier(alpha = 0.0001, loss='hinge', n_jobs=-1, penalty='l2'), \n",
    "                                    tf_idf, labels, scoring = 'accuracy').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LogisticRegression(),\n",
       "             param_grid={'C': [1, 0.1, 0.01], 'n_jobs': [-1],\n",
       "                         'penalty': ['l1', 'l2', 'elasticnet'],\n",
       "                         'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag',\n",
       "                                    'saga']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = {'penalty':['l1', 'l2','elasticnet'],'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], \n",
    "         'C':[1,0.1,0.01], 'n_jobs': [-1]}\n",
    "clf = GridSearchCV(estimator = LogisticRegression(), param_grid = param, scoring = 'accuracy')\n",
    "clf.fit(tf_idf, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit_tfidf:  0.6679999999999999\n"
     ]
    }
   ],
   "source": [
    "print('logit_tfidf: ',cross_val_score(LogisticRegression(solver='saga'), \n",
    "                                    tf_idf, labels, scoring = 'accuracy').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно уменьшить размер признаков и обучить нелинейные модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import NMF, TruncatedSVD\n",
    "\n",
    "mf = TruncatedSVD(2000)\n",
    "new_df = mf.fit_transform(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forest:  0.6565000000000001\n",
      "xgboost:  0.7005000000000001\n"
     ]
    }
   ],
   "source": [
    "print('forest: ',cross_val_score(RandomForestClassifier(300), new_df, labels, scoring = 'accuracy').mean())\n",
    "print('xgboost: ',cross_val_score(XGBClassifier(300), new_df, labels, scoring = 'accuracy').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И рассмотрим вариант с многослойной нейронной сеткой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_net = keras.Sequential([\n",
    "    keras.layers.Dense(256, activation='relu', input_shape=(46055,),kernel_regularizer=regularizers.l2(0.01)), \n",
    "    keras.layers.Dense(128, activation='relu'),   \n",
    "    keras.layers.Dense(64, activation='relu'),   \n",
    "    keras.layers.Dense(2, activation='softmax', activity_regularizer=regularizers.l1(0.01))\n",
    "                       ])\n",
    "\n",
    "model_net.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/20\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.9966 - accuracy: 0.6325 - val_loss: 0.8874 - val_accuracy: 0.6500\n",
      "Epoch 2/20\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.9322 - accuracy: 0.7769 - val_loss: 1.1059 - val_accuracy: 0.7350\n",
      "Epoch 3/20\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.8625 - accuracy: 0.8938 - val_loss: 1.2191 - val_accuracy: 0.7550\n",
      "Epoch 4/20\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.7204 - accuracy: 0.9413 - val_loss: 1.1570 - val_accuracy: 0.7625\n",
      "Epoch 5/20\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.5272 - accuracy: 0.9731 - val_loss: 1.0339 - val_accuracy: 0.7575\n",
      "Epoch 6/20\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.3790 - accuracy: 0.9856 - val_loss: 0.9881 - val_accuracy: 0.7475\n",
      "Epoch 7/20\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.3299 - accuracy: 0.9825 - val_loss: 1.0958 - val_accuracy: 0.7625\n",
      "Epoch 8/20\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.3546 - accuracy: 0.9812 - val_loss: 1.1001 - val_accuracy: 0.7525\n",
      "Epoch 9/20\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.3083 - accuracy: 0.9887 - val_loss: 1.1476 - val_accuracy: 0.7375\n",
      "Epoch 10/20\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.3042 - accuracy: 0.9800 - val_loss: 1.3062 - val_accuracy: 0.6875\n",
      "Epoch 11/20\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2982 - accuracy: 0.9875 - val_loss: 1.1096 - val_accuracy: 0.7850\n",
      "Epoch 12/20\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2443 - accuracy: 0.9956 - val_loss: 1.0042 - val_accuracy: 0.7875\n",
      "Epoch 13/20\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2263 - accuracy: 0.9894 - val_loss: 1.0916 - val_accuracy: 0.7750\n",
      "Epoch 14/20\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2331 - accuracy: 0.9900 - val_loss: 1.1665 - val_accuracy: 0.7525\n",
      "Epoch 15/20\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2278 - accuracy: 0.9962 - val_loss: 1.1499 - val_accuracy: 0.7350\n",
      "Epoch 16/20\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2084 - accuracy: 0.9906 - val_loss: 1.1074 - val_accuracy: 0.7425\n",
      "Epoch 17/20\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2160 - accuracy: 0.9937 - val_loss: 1.0893 - val_accuracy: 0.7475\n",
      "Epoch 18/20\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1966 - accuracy: 0.9944 - val_loss: 1.0607 - val_accuracy: 0.7725\n",
      "Epoch 19/20\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1764 - accuracy: 0.9950 - val_loss: 1.0417 - val_accuracy: 0.7625\n",
      "Epoch 20/20\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1973 - accuracy: 0.9856 - val_loss: 1.2540 - val_accuracy: 0.7600\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_net = keras.callbacks.ModelCheckpoint(\"mymodel1.hdf5\", monitor='val_accuracy', save_best_only=True, \n",
    "                                                 mode='max', period='epoch')\n",
    "history_net = model_net.fit(tf_idf.toarray(), df.iloc[:,-1], epochs=20, batch_size=16, validation_split=0.2, \n",
    "                            callbacks = [checkpoint_net])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_net.load_weights('mymodel1.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработаем нашу тестовую выборку и сделаем первую отсылку лучшей модели на проверку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem=WordNetLemmatizer()\n",
    "lem_test = test.iloc[:,0].values\n",
    "for i,text in enumerate(test.iloc[:,0].values):\n",
    "    text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n",
    "    words_lem=[lem.lemmatize(word) for word in text.split(' ')]\n",
    "    words_lem_ps = [ps.stem(word) for word in words_lem]  \n",
    "    sentiment =  ' '.join(words_lem_ps)\n",
    "    lem_test[i] = sentiment\n",
    "    \n",
    "tf_idf_test = tfv.transform(lem_test)\n",
    "new_test = mf.fit_transform(lem_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем результат 0.778.                                      \n",
    "Попробуем взять несколько моделей и проголовать большинством"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(solver='saga')\n",
    "svc = LinearSVC(C=2, loss='hinge',random_state = 42)\n",
    "sgd = SGDClassifier(alpha = 0.0001, loss='hinge', n_jobs=-1, penalty='l2')\n",
    "xgboost = XGBClassifier(300)\n",
    "\n",
    "logit.fit(tf_idf, labels)\n",
    "svc.fit(tf_idf, labels)\n",
    "sgd.fit(tf_idf, labels)\n",
    "xgboost.fit(new_df, labels)\n",
    "\n",
    "sgd_pred = sgd.predict(tf_idf_test)\n",
    "logit_pred = logit.predict(tf_idf_test)\n",
    "svc_pred = svc.predict(tf_idf_test)\n",
    "xgb_pred = xgboost.predict(new_test)\n",
    "net = model_net.predict(new_test)\n",
    "net_pred = [0 if i< 0.5 else 1 for i in net[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred = pd.DataFrame()\n",
    "\n",
    "all_pred['sgd'] = sgd_pred\n",
    "all_pred['logit'] = logit_pred\n",
    "all_pred['svc'] = svc_pred\n",
    "all_pred['net'] = net_pred\n",
    "all_pred['xgb'] = xgb_pred\n",
    "\n",
    "all_model = all_pred.sum(axis=1)\n",
    "answer = [0 if i < 3 else 1 for i in all_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "example1 = pd.read_csv(\"products_sentiment_sample_submission.csv\")\n",
    "example1['y'] = answer\n",
    "example1.to_csv('submission_ton.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем результат 0.791"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec и сверточная нейронная сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem=WordNetLemmatizer()\n",
    "ps =PorterStemmer()\n",
    "lem_reviews_cnn = np.array(df.iloc[:,0].values)\n",
    "\n",
    "for i,text in enumerate(df.iloc[:,0].values):\n",
    "    text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n",
    "    words_lem=[lem.lemmatize(word) for word in text.split(' ')]\n",
    "    words_lem_ps = [ps.stem(word) for word in words_lem]   \n",
    "#     sentiment =  ' '.join(words_lem_ps)\n",
    "    lem_reviews_cnn[i] = words_lem_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Высота матрицы (максимальное количество слов в отзыве)\n",
    "SENTENCE_LENGTH = 79\n",
    "# Размер словаря\n",
    "NUM = 500000\n",
    "\n",
    "def get_sequences(tokenizer, x):\n",
    "    sequences = tokenizer.texts_to_sequences(x)\n",
    "    return pad_sequences(sequences, maxlen=SENTENCE_LENGTH)\n",
    "\n",
    "# Cоздаем и обучаем токенизатор\n",
    "tokenizer = Tokenizer(num_words=NUM)\n",
    "tokenizer.fit_on_texts(lem_reviews_cnn)\n",
    "\n",
    "# Отображаем каждый текст в массив идентификаторов токенов\n",
    "x_train_seq = get_sequences(tokenizer, lem_reviews_cnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(704206, 1115580)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vec = Word2Vec(size=300, window=5, min_count=3)\n",
    "model_vec.build_vocab(lem_reviews_cnn)\n",
    "model_vec.train(lem_reviews_cnn, total_examples=model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = model.vector_size \n",
    "# Инициализируем матрицу embedding слоя нулями\n",
    "embedding_matrix = np.zeros((NUM, DIM))\n",
    "# Добавляем NUM=100000 наиболее часто встречающихся слов из обучающей выборки в embedding слой\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= NUM:\n",
    "        break\n",
    "    if word in model.wv.vocab.keys():\n",
    "        embedding_matrix[i] = model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_input = keras.layers.Input(shape=(SENTENCE_LENGTH,), dtype='int32')\n",
    "tweet_encoder = keras.layers.Embedding(NUM, DIM, input_length=SENTENCE_LENGTH,\n",
    "                          weights=[embedding_matrix], trainable=True)(tweet_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "branches = []\n",
    "# Добавляем dropout-регуляризацию\n",
    "x = keras.layers.Dropout(0.2)(tweet_encoder)\n",
    "\n",
    "for size, filters_count in [(2, 10), (3, 10), (4, 10), (5, 10)]:\n",
    "    for i in range(filters_count):\n",
    "        # Добавляем слой свертки\n",
    "        branch = keras.layers.Conv1D(filters=1, kernel_size=size, padding='valid', activation='relu')(x)\n",
    "        # Добавляем слой субдискретизации\n",
    "        branch = keras.layers.GlobalMaxPooling1D()(branch)\n",
    "        branches.append(branch)\n",
    "# Конкатенируем карты признаков\n",
    "x = keras.layers.concatenate(branches, axis=1)\n",
    "# Добавляем dropout-регуляризацию\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Dense(30, activation='relu')(x)\n",
    "x = keras.layers.Dense(1)(x)\n",
    "output = keras.layers.Activation('sigmoid')(x)\n",
    "\n",
    "model_cnn = keras.models.Model(inputs=[tweet_input], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 300, 300)     150000000   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 300, 300)     0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 299, 1)       601         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 299, 1)       601         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 299, 1)       601         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 299, 1)       601         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 299, 1)       601         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 299, 1)       601         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 299, 1)       601         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 299, 1)       601         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 299, 1)       601         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 299, 1)       601         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 298, 1)       901         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 298, 1)       901         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 298, 1)       901         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 298, 1)       901         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 298, 1)       901         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 298, 1)       901         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 298, 1)       901         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 298, 1)       901         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 298, 1)       901         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 298, 1)       901         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 297, 1)       1201        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 297, 1)       1201        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 297, 1)       1201        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 297, 1)       1201        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 297, 1)       1201        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 297, 1)       1201        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 297, 1)       1201        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 297, 1)       1201        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 297, 1)       1201        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 297, 1)       1201        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 296, 1)       1501        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 296, 1)       1501        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 296, 1)       1501        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 296, 1)       1501        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 296, 1)       1501        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 296, 1)       1501        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 296, 1)       1501        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 296, 1)       1501        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 296, 1)       1501        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 296, 1)       1501        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 1)            0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 1)            0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 1)            0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 1)            0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 1)            0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 1)            0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 1)            0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 1)            0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 1)            0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 1)            0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 1)            0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 1)            0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 1)            0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 1)            0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 1)            0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 1)            0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 1)            0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 1)            0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 1)            0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 1)            0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 1)            0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 1)            0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 1)            0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 1)            0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_24 (Global (None, 1)            0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_25 (Global (None, 1)            0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_26 (Global (None, 1)            0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_27 (Global (None, 1)            0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_28 (Global (None, 1)            0           conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_29 (Global (None, 1)            0           conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_30 (Global (None, 1)            0           conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_31 (Global (None, 1)            0           conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_32 (Global (None, 1)            0           conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_33 (Global (None, 1)            0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_34 (Global (None, 1)            0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_35 (Global (None, 1)            0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_36 (Global (None, 1)            0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_37 (Global (None, 1)            0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_38 (Global (None, 1)            0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_39 (Global (None, 1)            0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 40)           0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "                                                                 global_max_pooling1d_10[0][0]    \n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "                                                                 global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "                                                                 global_max_pooling1d_14[0][0]    \n",
      "                                                                 global_max_pooling1d_15[0][0]    \n",
      "                                                                 global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "                                                                 global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "                                                                 global_max_pooling1d_20[0][0]    \n",
      "                                                                 global_max_pooling1d_21[0][0]    \n",
      "                                                                 global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "                                                                 global_max_pooling1d_24[0][0]    \n",
      "                                                                 global_max_pooling1d_25[0][0]    \n",
      "                                                                 global_max_pooling1d_26[0][0]    \n",
      "                                                                 global_max_pooling1d_27[0][0]    \n",
      "                                                                 global_max_pooling1d_28[0][0]    \n",
      "                                                                 global_max_pooling1d_29[0][0]    \n",
      "                                                                 global_max_pooling1d_30[0][0]    \n",
      "                                                                 global_max_pooling1d_31[0][0]    \n",
      "                                                                 global_max_pooling1d_32[0][0]    \n",
      "                                                                 global_max_pooling1d_33[0][0]    \n",
      "                                                                 global_max_pooling1d_34[0][0]    \n",
      "                                                                 global_max_pooling1d_35[0][0]    \n",
      "                                                                 global_max_pooling1d_36[0][0]    \n",
      "                                                                 global_max_pooling1d_37[0][0]    \n",
      "                                                                 global_max_pooling1d_38[0][0]    \n",
      "                                                                 global_max_pooling1d_39[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 40)           0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 30)           1230        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            31          dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 1)            0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 150,043,301\n",
      "Trainable params: 150,043,301\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 216s 135ms/sample - loss: 0.6160 - accuracy: 0.6450 - val_loss: 0.5468 - val_accuracy: 0.7250\n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 166s 104ms/sample - loss: 0.5574 - accuracy: 0.7000 - val_loss: 0.5479 - val_accuracy: 0.7100\n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 187s 117ms/sample - loss: 0.5085 - accuracy: 0.7431 - val_loss: 0.5112 - val_accuracy: 0.7500\n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 199s 124ms/sample - loss: 0.4492 - accuracy: 0.7862 - val_loss: 0.4772 - val_accuracy: 0.7625\n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 164s 102ms/sample - loss: 0.3690 - accuracy: 0.8456 - val_loss: 0.4733 - val_accuracy: 0.7575\n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 190s 119ms/sample - loss: 0.2778 - accuracy: 0.8938 - val_loss: 0.4606 - val_accuracy: 0.7775\n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 196s 123ms/sample - loss: 0.1969 - accuracy: 0.9319 - val_loss: 0.4815 - val_accuracy: 0.7900\n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 163s 102ms/sample - loss: 0.1347 - accuracy: 0.9613 - val_loss: 0.5135 - val_accuracy: 0.7775\n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 195s 122ms/sample - loss: 0.0963 - accuracy: 0.9675 - val_loss: 0.5268 - val_accuracy: 0.7950\n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 170s 106ms/sample - loss: 0.0574 - accuracy: 0.9856 - val_loss: 0.5973 - val_accuracy: 0.7725\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\"mymodel.hdf5\", monitor='val_accuracy', save_best_only=True, \n",
    "                                             mode='max', period='epoch')\n",
    "history = model_cnn.fit(x_train_seq, labels, batch_size=32, epochs=10, validation_split=0.2, callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.load_weights('mymodel.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим что получается на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem=WordNetLemmatizer()\n",
    "lem_test_cnn = test.iloc[:,0].values\n",
    "for i,text in enumerate(test.iloc[:,0].values):\n",
    "    text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n",
    "    words_lem=[lem.lemmatize(word) for word in text.split(' ')]\n",
    "    words_lem_ps = [ps.stem(word) for word in words_lem]  \n",
    "#     sentiment =  ' '.join(words_lem_ps)\n",
    "    lem_test_cnn[i] = words_lem_ps\n",
    "    \n",
    "x_test_seq = get_sequences(tokenizer, lem_test_cnn)\n",
    "\n",
    "predicted = model_cnn.predict(x_test_seq)\n",
    "cnn_pred = [0 if i < 0.5 else 1 for i in predicted]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили результат 0.7822"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим несколько моделей на новом варианте обработаных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))\n",
    "\n",
    "vec_text=[]\n",
    "for text in lem_reviews_cnn:\n",
    "    vec_word=[]\n",
    "    for word in text:\n",
    "        if word in w2v:\n",
    "            vec_word.append(w2v[word])\n",
    "    mean_vec_text = np.sum(vec_word,axis=0)\n",
    "    vec_text.append(mean_vec_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc_tfidf:  0.7070000000000001\n"
     ]
    }
   ],
   "source": [
    "print('svc_tfidf: ',cross_val_score(RandomForestClassifier(300), vec_text, \n",
    "                                    df.iloc[:,-1], scoring = 'accuracy').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgd_tfidf:  0.7015\n"
     ]
    }
   ],
   "source": [
    "print('sgd_tfidf: ',cross_val_score(SGDClassifier(alpha = 0.0001, n_jobs=-1, penalty='l2'), \n",
    "                                    vec_text, df.iloc[:,-1], scoring = 'accuracy').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат получился хуже, чем был.                      \n",
    "Но если добавить нашу новую модель сверточной сети к тем что уже были получены раньше, то получим следующие результаты на тесте:\n",
    "\n",
    "sgd + cnn + net + svc + xgb = 0.813                          \n",
    "cnn + net + xgb = 0.780                            \n",
    "xgb + cnn + svc = 0.815"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (DistilBertModel, DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "# Загрузка предобученной модели/токенизатора \n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразуем каждый отзыв в список идентификаторов.\n",
    "tokenized = df.iloc[:,0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#добавим в конец нули, чтобы выровнять по длине отзывы\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим входной вектор из матрицы токенов и передадим его в DistilBERT\n",
    "input_ids = torch.tensor(np.array(padded)).to(torch.int64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Разрежем выход для первой позиции во всех последовательностях, возьмем все выходы скрытых нейронок \n",
    "features = last_hidden_states[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще раз подберем параметры обучающих моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'penalty':['l1', 'l2','elasticnet'], 'loss':['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'], \n",
    "         'alpha':[0.0001,0.001,0.01], 'n_jobs': [-1]}\n",
    "clf = GridSearchCV(estimator = SGDClassifier(random_state = 42), param_grid = param, scoring = 'accuracy')\n",
    "clf.fit(features, labels)\n",
    "best_sgd = clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'penalty':['l1', 'l2','elasticnet'],'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], \n",
    "         'C':[1,0.1,0.01], 'n_jobs': [-1]}\n",
    "clf = GridSearchCV(estimator = LogisticRegression(), param_grid = param, scoring = 'accuracy')\n",
    "clf.fit(features, labels)\n",
    "best_logit = clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'penalty':['l1', 'l2'], 'loss':['hinge','squared_hinge'], 'C':[1,2,3]}\n",
    "clf = GridSearchCV(LinearSVC(random_state = 42), param, scoring = 'accuracy')\n",
    "clf.fit(features, labels)\n",
    "best_svc = clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc_bert:  0.834\n",
      "sgd_bert:  0.8240000000000001\n",
      "logit_bert:  0.8230000000000001\n",
      "xgboost_bert:  0.772\n",
      "forest_bert:  0.748\n"
     ]
    }
   ],
   "source": [
    "print('svc_bert: ',cross_val_score(LinearSVC(C=2, loss='hinge',random_state = 42), features, labels, \n",
    "                                   scoring = 'accuracy').mean())\n",
    "\n",
    "print('sgd_bert: ',cross_val_score(SGDClassifier(alpha = 0.0001, loss = 'modified_huber', n_jobs=-1, penalty='l1'), \n",
    "                                    features, labels, scoring = 'accuracy').mean())\n",
    "\n",
    "print('logit_bert: ',cross_val_score(LogisticRegression(C=1, solver='liblinear'), \n",
    "                                    features, labels, scoring = 'accuracy').mean())\n",
    "\n",
    "print('xgboost_bert: ',cross_val_score(XGBClassifier(300), \n",
    "                                    features, labels, scoring = 'accuracy').mean())\n",
    "\n",
    "print('forest_bert: ',cross_val_score(RandomForestClassifier(300), \n",
    "                                    features, labels, scoring = 'accuracy').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем хороший результат "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_net = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation='relu', input_shape=(768,)),  \n",
    "    keras.layers.Dense(64, activation='relu'),   \n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(2, activation='softmax')\n",
    "                       ])\n",
    "\n",
    "\n",
    "model_net.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/50\n",
      "1600/1600 [==============================] - ETA: 23s - loss: 0.6370 - accuracy: 0.625 - ETA: 0s - loss: 0.6650 - accuracy: 0.590 - ETA: 0s - loss: 0.6604 - accuracy: 0.59 - ETA: 0s - loss: 0.6480 - accuracy: 0.62 - 1s 399us/sample - loss: 0.6456 - accuracy: 0.6263 - val_loss: 0.6023 - val_accuracy: 0.7075\n",
      "Epoch 2/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.6774 - accuracy: 0.50 - ETA: 0s - loss: 0.6078 - accuracy: 0.65 - ETA: 0s - loss: 0.6231 - accuracy: 0.65 - ETA: 0s - loss: 0.6167 - accuracy: 0.66 - 0s 161us/sample - loss: 0.6106 - accuracy: 0.6681 - val_loss: 0.6040 - val_accuracy: 0.7000\n",
      "Epoch 3/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.7032 - accuracy: 0.43 - ETA: 0s - loss: 0.6376 - accuracy: 0.64 - ETA: 0s - loss: 0.6233 - accuracy: 0.65 - ETA: 0s - loss: 0.6105 - accuracy: 0.68 - ETA: 0s - loss: 0.5940 - accuracy: 0.69 - ETA: 0s - loss: 0.5820 - accuracy: 0.70 - ETA: 0s - loss: 0.5809 - accuracy: 0.70 - 1s 327us/sample - loss: 0.5725 - accuracy: 0.7113 - val_loss: 0.5159 - val_accuracy: 0.7725\n",
      "Epoch 4/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.4203 - accuracy: 0.93 - ETA: 0s - loss: 0.5553 - accuracy: 0.71 - ETA: 0s - loss: 0.5389 - accuracy: 0.74 - ETA: 0s - loss: 0.5214 - accuracy: 0.75 - ETA: 0s - loss: 0.5317 - accuracy: 0.75 - 0s 159us/sample - loss: 0.5333 - accuracy: 0.7456 - val_loss: 0.5358 - val_accuracy: 0.7425\n",
      "Epoch 5/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.4425 - accuracy: 0.87 - ETA: 0s - loss: 0.5318 - accuracy: 0.76 - ETA: 0s - loss: 0.5114 - accuracy: 0.76 - ETA: 0s - loss: 0.5090 - accuracy: 0.75 - 0s 178us/sample - loss: 0.4951 - accuracy: 0.7638 - val_loss: 0.4600 - val_accuracy: 0.8150\n",
      "Epoch 6/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.3777 - accuracy: 0.93 - ETA: 0s - loss: 0.4860 - accuracy: 0.76 - ETA: 0s - loss: 0.4750 - accuracy: 0.78 - ETA: 0s - loss: 0.4627 - accuracy: 0.77 - 0s 129us/sample - loss: 0.4654 - accuracy: 0.7806 - val_loss: 0.4486 - val_accuracy: 0.8050\n",
      "Epoch 7/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.2254 - accuracy: 0.93 - ETA: 0s - loss: 0.4745 - accuracy: 0.78 - ETA: 0s - loss: 0.4566 - accuracy: 0.79 - ETA: 0s - loss: 0.4515 - accuracy: 0.79 - ETA: 0s - loss: 0.4508 - accuracy: 0.80 - 0s 154us/sample - loss: 0.4525 - accuracy: 0.8000 - val_loss: 0.4804 - val_accuracy: 0.7650\n",
      "Epoch 8/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.5200 - accuracy: 0.75 - ETA: 0s - loss: 0.4881 - accuracy: 0.78 - ETA: 0s - loss: 0.4746 - accuracy: 0.77 - ETA: 0s - loss: 0.4509 - accuracy: 0.79 - 0s 136us/sample - loss: 0.4538 - accuracy: 0.7856 - val_loss: 0.4492 - val_accuracy: 0.7925\n",
      "Epoch 9/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.2908 - accuracy: 0.87 - ETA: 0s - loss: 0.4086 - accuracy: 0.81 - ETA: 0s - loss: 0.4265 - accuracy: 0.79 - ETA: 0s - loss: 0.4193 - accuracy: 0.80 - 0s 131us/sample - loss: 0.4327 - accuracy: 0.8037 - val_loss: 0.5133 - val_accuracy: 0.7625\n",
      "Epoch 10/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.5328 - accuracy: 0.75 - ETA: 0s - loss: 0.4585 - accuracy: 0.79 - ETA: 0s - loss: 0.4305 - accuracy: 0.80 - ETA: 0s - loss: 0.4386 - accuracy: 0.79 - 0s 145us/sample - loss: 0.4325 - accuracy: 0.7956 - val_loss: 0.4333 - val_accuracy: 0.8050\n",
      "Epoch 11/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.6133 - accuracy: 0.68 - ETA: 0s - loss: 0.4115 - accuracy: 0.80 - ETA: 0s - loss: 0.4242 - accuracy: 0.79 - ETA: 0s - loss: 0.4295 - accuracy: 0.79 - ETA: 0s - loss: 0.4289 - accuracy: 0.79 - 0s 163us/sample - loss: 0.4284 - accuracy: 0.7981 - val_loss: 0.4296 - val_accuracy: 0.8075\n",
      "Epoch 12/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.2961 - accuracy: 0.93 - ETA: 0s - loss: 0.3418 - accuracy: 0.85 - ETA: 0s - loss: 0.4232 - accuracy: 0.80 - ETA: 0s - loss: 0.3900 - accuracy: 0.83 - ETA: 0s - loss: 0.3931 - accuracy: 0.82 - ETA: 0s - loss: 0.3979 - accuracy: 0.82 - 0s 211us/sample - loss: 0.4032 - accuracy: 0.8175 - val_loss: 0.4548 - val_accuracy: 0.7750\n",
      "Epoch 13/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.8273 - accuracy: 0.68 - ETA: 0s - loss: 0.4226 - accuracy: 0.80 - ETA: 0s - loss: 0.3924 - accuracy: 0.82 - ETA: 0s - loss: 0.4374 - accuracy: 0.79 - ETA: 0s - loss: 0.4231 - accuracy: 0.80 - ETA: 0s - loss: 0.4256 - accuracy: 0.80 - 0s 191us/sample - loss: 0.4247 - accuracy: 0.8106 - val_loss: 0.4253 - val_accuracy: 0.8125\n",
      "Epoch 14/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.4326 - accuracy: 0.81 - ETA: 0s - loss: 0.3938 - accuracy: 0.81 - ETA: 0s - loss: 0.3671 - accuracy: 0.83 - ETA: 0s - loss: 0.3790 - accuracy: 0.83 - ETA: 0s - loss: 0.4008 - accuracy: 0.81 - 0s 259us/sample - loss: 0.3932 - accuracy: 0.8225 - val_loss: 0.4425 - val_accuracy: 0.8200\n",
      "Epoch 15/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.4615 - accuracy: 0.81 - ETA: 0s - loss: 0.3882 - accuracy: 0.82 - ETA: 0s - loss: 0.4216 - accuracy: 0.80 - ETA: 0s - loss: 0.4110 - accuracy: 0.82 - ETA: 0s - loss: 0.4274 - accuracy: 0.80 - 0s 173us/sample - loss: 0.4197 - accuracy: 0.8144 - val_loss: 0.4530 - val_accuracy: 0.7825\n",
      "Epoch 16/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.4720 - accuracy: 0.81 - ETA: 0s - loss: 0.4015 - accuracy: 0.81 - ETA: 0s - loss: 0.4090 - accuracy: 0.81 - ETA: 0s - loss: 0.4082 - accuracy: 0.82 - 0s 131us/sample - loss: 0.4063 - accuracy: 0.8194 - val_loss: 0.4333 - val_accuracy: 0.7950\n",
      "Epoch 17/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.3846 - accuracy: 0.87 - ETA: 0s - loss: 0.3688 - accuracy: 0.84 - ETA: 0s - loss: 0.3738 - accuracy: 0.84 - ETA: 0s - loss: 0.3883 - accuracy: 0.83 - 0s 127us/sample - loss: 0.3828 - accuracy: 0.8381 - val_loss: 0.4988 - val_accuracy: 0.7400\n",
      "Epoch 18/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.7457 - accuracy: 0.68 - ETA: 0s - loss: 0.4228 - accuracy: 0.82 - ETA: 0s - loss: 0.4098 - accuracy: 0.82 - ETA: 0s - loss: 0.3946 - accuracy: 0.83 - 0s 218us/sample - loss: 0.3848 - accuracy: 0.8363 - val_loss: 0.4218 - val_accuracy: 0.8225\n",
      "Epoch 19/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.4454 - accuracy: 0.81 - ETA: 0s - loss: 0.3731 - accuracy: 0.84 - ETA: 0s - loss: 0.3804 - accuracy: 0.83 - ETA: 0s - loss: 0.3763 - accuracy: 0.83 - ETA: 0s - loss: 0.3783 - accuracy: 0.83 - 0s 156us/sample - loss: 0.3818 - accuracy: 0.8313 - val_loss: 0.4880 - val_accuracy: 0.7775\n",
      "Epoch 20/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.2943 - accuracy: 0.75 - ETA: 0s - loss: 0.4544 - accuracy: 0.79 - ETA: 0s - loss: 0.3966 - accuracy: 0.82 - ETA: 0s - loss: 0.3762 - accuracy: 0.83 - 0s 144us/sample - loss: 0.3694 - accuracy: 0.8375 - val_loss: 0.4321 - val_accuracy: 0.8125\n",
      "Epoch 21/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.2740 - accuracy: 0.87 - ETA: 0s - loss: 0.3952 - accuracy: 0.81 - ETA: 0s - loss: 0.3686 - accuracy: 0.83 - ETA: 0s - loss: 0.3717 - accuracy: 0.83 - 0s 133us/sample - loss: 0.3655 - accuracy: 0.8394 - val_loss: 0.4821 - val_accuracy: 0.7850\n",
      "Epoch 22/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.4596 - accuracy: 0.68 - ETA: 0s - loss: 0.3696 - accuracy: 0.84 - ETA: 0s - loss: 0.3374 - accuracy: 0.86 - ETA: 0s - loss: 0.3639 - accuracy: 0.84 - 0s 136us/sample - loss: 0.3808 - accuracy: 0.8394 - val_loss: 0.6321 - val_accuracy: 0.6500\n",
      "Epoch 23/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.5691 - accuracy: 0.75 - ETA: 0s - loss: 0.4386 - accuracy: 0.80 - ETA: 0s - loss: 0.4340 - accuracy: 0.79 - ETA: 0s - loss: 0.4185 - accuracy: 0.79 - ETA: 0s - loss: 0.4063 - accuracy: 0.80 - ETA: 0s - loss: 0.3878 - accuracy: 0.81 - ETA: 0s - loss: 0.3752 - accuracy: 0.82 - 1s 369us/sample - loss: 0.3805 - accuracy: 0.8244 - val_loss: 0.4131 - val_accuracy: 0.8275\n",
      "Epoch 24/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.2348 - accuracy: 0.93 - ETA: 0s - loss: 0.3542 - accuracy: 0.85 - ETA: 0s - loss: 0.3447 - accuracy: 0.85 - ETA: 0s - loss: 0.3506 - accuracy: 0.84 - ETA: 0s - loss: 0.3635 - accuracy: 0.83 - 0s 168us/sample - loss: 0.3668 - accuracy: 0.8344 - val_loss: 0.4319 - val_accuracy: 0.8150\n",
      "Epoch 25/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.4669 - accuracy: 0.75 - ETA: 0s - loss: 0.3883 - accuracy: 0.83 - ETA: 0s - loss: 0.3855 - accuracy: 0.82 - ETA: 0s - loss: 0.3719 - accuracy: 0.83 - 0s 171us/sample - loss: 0.3677 - accuracy: 0.8388 - val_loss: 0.4063 - val_accuracy: 0.8325\n",
      "Epoch 26/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.4186 - accuracy: 0.93 - ETA: 0s - loss: 0.3523 - accuracy: 0.85 - ETA: 0s - loss: 0.3425 - accuracy: 0.84 - ETA: 0s - loss: 0.3694 - accuracy: 0.83 - 0s 133us/sample - loss: 0.3775 - accuracy: 0.8294 - val_loss: 0.4618 - val_accuracy: 0.7825\n",
      "Epoch 27/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.2123 - accuracy: 0.87 - ETA: 0s - loss: 0.3304 - accuracy: 0.85 - ETA: 0s - loss: 0.3457 - accuracy: 0.84 - ETA: 0s - loss: 0.3527 - accuracy: 0.83 - ETA: 0s - loss: 0.3423 - accuracy: 0.85 - 0s 153us/sample - loss: 0.3433 - accuracy: 0.8519 - val_loss: 0.4158 - val_accuracy: 0.8275\n",
      "Epoch 28/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.1064 - accuracy: 1.00 - ETA: 0s - loss: 0.3161 - accuracy: 0.86 - ETA: 0s - loss: 0.3383 - accuracy: 0.85 - ETA: 0s - loss: 0.3447 - accuracy: 0.85 - 0s 123us/sample - loss: 0.3408 - accuracy: 0.8550 - val_loss: 0.4083 - val_accuracy: 0.8325\n",
      "Epoch 29/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.2301 - accuracy: 0.87 - ETA: 0s - loss: 0.2961 - accuracy: 0.86 - ETA: 0s - loss: 0.3323 - accuracy: 0.84 - ETA: 0s - loss: 0.3551 - accuracy: 0.83 - 0s 123us/sample - loss: 0.3541 - accuracy: 0.8400 - val_loss: 0.4067 - val_accuracy: 0.8150\n",
      "Epoch 30/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.3602 - accuracy: 0.75 - ETA: 2s - loss: 0.2580 - accuracy: 0.87 - ETA: 0s - loss: 0.2787 - accuracy: 0.90 - ETA: 0s - loss: 0.3178 - accuracy: 0.87 - ETA: 0s - loss: 0.3164 - accuracy: 0.87 - ETA: 0s - loss: 0.3320 - accuracy: 0.85 - 0s 193us/sample - loss: 0.3352 - accuracy: 0.8569 - val_loss: 0.3998 - val_accuracy: 0.8150\n",
      "Epoch 31/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.2782 - accuracy: 0.93 - ETA: 0s - loss: 0.3299 - accuracy: 0.87 - ETA: 0s - loss: 0.3251 - accuracy: 0.87 - ETA: 0s - loss: 0.3319 - accuracy: 0.87 - 0s 117us/sample - loss: 0.3337 - accuracy: 0.8712 - val_loss: 0.4842 - val_accuracy: 0.7450\n",
      "Epoch 32/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.2874 - accuracy: 0.93 - ETA: 0s - loss: 0.3431 - accuracy: 0.85 - ETA: 0s - loss: 0.3361 - accuracy: 0.86 - ETA: 0s - loss: 0.3376 - accuracy: 0.86 - 0s 134us/sample - loss: 0.3195 - accuracy: 0.8737 - val_loss: 0.4647 - val_accuracy: 0.8175\n",
      "Epoch 33/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.5757 - accuracy: 0.75 - ETA: 0s - loss: 0.3195 - accuracy: 0.86 - ETA: 0s - loss: 0.3306 - accuracy: 0.86 - ETA: 0s - loss: 0.3546 - accuracy: 0.84 - 0s 121us/sample - loss: 0.3526 - accuracy: 0.8506 - val_loss: 0.4644 - val_accuracy: 0.7850\n",
      "Epoch 34/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.6291 - accuracy: 0.68 - ETA: 0s - loss: 0.3290 - accuracy: 0.84 - ETA: 0s - loss: 0.3883 - accuracy: 0.82 - ETA: 0s - loss: 0.3789 - accuracy: 0.82 - 0s 119us/sample - loss: 0.3778 - accuracy: 0.8294 - val_loss: 0.4226 - val_accuracy: 0.8050\n",
      "Epoch 35/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.2630 - accuracy: 0.87 - ETA: 0s - loss: 0.3542 - accuracy: 0.84 - ETA: 0s - loss: 0.3219 - accuracy: 0.86 - ETA: 0s - loss: 0.3235 - accuracy: 0.86 - 0s 128us/sample - loss: 0.3278 - accuracy: 0.8600 - val_loss: 0.4144 - val_accuracy: 0.8225\n",
      "Epoch 36/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.2292 - accuracy: 0.93 - ETA: 0s - loss: 0.2812 - accuracy: 0.87 - ETA: 0s - loss: 0.3157 - accuracy: 0.86 - ETA: 0s - loss: 0.3239 - accuracy: 0.86 - 0s 123us/sample - loss: 0.3304 - accuracy: 0.8594 - val_loss: 0.4311 - val_accuracy: 0.8000\n",
      "Epoch 37/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.1972 - accuracy: 0.93 - ETA: 0s - loss: 0.3832 - accuracy: 0.83 - ETA: 0s - loss: 0.3514 - accuracy: 0.84 - ETA: 0s - loss: 0.3543 - accuracy: 0.84 - 0s 143us/sample - loss: 0.3550 - accuracy: 0.8425 - val_loss: 0.4655 - val_accuracy: 0.7700\n",
      "Epoch 38/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.3038 - accuracy: 0.81 - ETA: 0s - loss: 0.3297 - accuracy: 0.85 - ETA: 0s - loss: 0.3208 - accuracy: 0.86 - ETA: 0s - loss: 0.3301 - accuracy: 0.85 - 0s 127us/sample - loss: 0.3219 - accuracy: 0.8631 - val_loss: 0.4263 - val_accuracy: 0.8175\n",
      "Epoch 39/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.1863 - accuracy: 1.00 - ETA: 0s - loss: 0.3327 - accuracy: 0.85 - ETA: 0s - loss: 0.3216 - accuracy: 0.86 - 0s 118us/sample - loss: 0.3150 - accuracy: 0.8706 - val_loss: 0.5509 - val_accuracy: 0.7375\n",
      "Epoch 40/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.4621 - accuracy: 0.81 - ETA: 0s - loss: 0.2860 - accuracy: 0.87 - ETA: 0s - loss: 0.3018 - accuracy: 0.87 - ETA: 0s - loss: 0.3235 - accuracy: 0.85 - 0s 119us/sample - loss: 0.3258 - accuracy: 0.8537 - val_loss: 0.6147 - val_accuracy: 0.6925\n",
      "Epoch 41/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.6861 - accuracy: 0.81 - ETA: 0s - loss: 0.3739 - accuracy: 0.84 - ETA: 0s - loss: 0.3843 - accuracy: 0.84 - ETA: 0s - loss: 0.3550 - accuracy: 0.85 - 0s 124us/sample - loss: 0.3448 - accuracy: 0.8575 - val_loss: 0.4498 - val_accuracy: 0.7975\n",
      "Epoch 42/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.2852 - accuracy: 0.87 - ETA: 0s - loss: 0.3565 - accuracy: 0.84 - ETA: 0s - loss: 0.3342 - accuracy: 0.85 - ETA: 0s - loss: 0.3225 - accuracy: 0.86 - 0s 120us/sample - loss: 0.3206 - accuracy: 0.8687 - val_loss: 0.4901 - val_accuracy: 0.7925\n",
      "Epoch 43/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.1638 - accuracy: 0.93 - ETA: 0s - loss: 0.3310 - accuracy: 0.84 - ETA: 0s - loss: 0.3074 - accuracy: 0.87 - ETA: 0s - loss: 0.2993 - accuracy: 0.88 - ETA: 0s - loss: 0.3034 - accuracy: 0.87 - 0s 163us/sample - loss: 0.3005 - accuracy: 0.8813 - val_loss: 0.4205 - val_accuracy: 0.8100\n",
      "Epoch 44/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.3538 - accuracy: 0.87 - ETA: 0s - loss: 0.3427 - accuracy: 0.84 - ETA: 0s - loss: 0.3371 - accuracy: 0.84 - ETA: 0s - loss: 0.3216 - accuracy: 0.85 - 0s 129us/sample - loss: 0.3219 - accuracy: 0.8581 - val_loss: 0.4394 - val_accuracy: 0.8100\n",
      "Epoch 45/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.2418 - accuracy: 0.81 - ETA: 0s - loss: 0.2799 - accuracy: 0.89 - ETA: 0s - loss: 0.2681 - accuracy: 0.89 - ETA: 0s - loss: 0.2858 - accuracy: 0.88 - 0s 123us/sample - loss: 0.2895 - accuracy: 0.8775 - val_loss: 0.4835 - val_accuracy: 0.7750\n",
      "Epoch 46/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.3833 - accuracy: 0.87 - ETA: 0s - loss: 0.2726 - accuracy: 0.89 - ETA: 0s - loss: 0.2906 - accuracy: 0.89 - ETA: 0s - loss: 0.2895 - accuracy: 0.89 - ETA: 0s - loss: 0.3013 - accuracy: 0.88 - 0s 167us/sample - loss: 0.3010 - accuracy: 0.8831 - val_loss: 0.5820 - val_accuracy: 0.7625\n",
      "Epoch 47/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.6669 - accuracy: 0.68 - ETA: 0s - loss: 0.3786 - accuracy: 0.81 - ETA: 0s - loss: 0.3500 - accuracy: 0.84 - ETA: 0s - loss: 0.3308 - accuracy: 0.85 - ETA: 0s - loss: 0.3242 - accuracy: 0.85 - ETA: 0s - loss: 0.3191 - accuracy: 0.86 - ETA: 0s - loss: 0.3204 - accuracy: 0.86 - 0s 238us/sample - loss: 0.3218 - accuracy: 0.8575 - val_loss: 0.4307 - val_accuracy: 0.8000\n",
      "Epoch 48/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.3856 - accuracy: 0.81 - ETA: 0s - loss: 0.4350 - accuracy: 0.79 - ETA: 0s - loss: 0.3731 - accuracy: 0.83 - ETA: 0s - loss: 0.3684 - accuracy: 0.83 - ETA: 0s - loss: 0.3464 - accuracy: 0.85 - 0s 168us/sample - loss: 0.3436 - accuracy: 0.8537 - val_loss: 0.4996 - val_accuracy: 0.7925\n",
      "Epoch 49/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.6891 - accuracy: 0.62 - ETA: 0s - loss: 0.3867 - accuracy: 0.83 - ETA: 0s - loss: 0.3518 - accuracy: 0.84 - ETA: 0s - loss: 0.3405 - accuracy: 0.85 - 0s 124us/sample - loss: 0.3391 - accuracy: 0.8537 - val_loss: 0.4255 - val_accuracy: 0.8250\n",
      "Epoch 50/50\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.1084 - accuracy: 1.00 - ETA: 0s - loss: 0.2902 - accuracy: 0.87 - ETA: 0s - loss: 0.3028 - accuracy: 0.86 - ETA: 0s - loss: 0.3009 - accuracy: 0.87 - 0s 124us/sample - loss: 0.2986 - accuracy: 0.8763 - val_loss: 0.4404 - val_accuracy: 0.8225\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint                    \n",
    "\n",
    "checkpoint_net = keras.callbacks.ModelCheckpoint(\"mymodel1.hdf5\", monitor='val_accuracy', save_best_only=True, \n",
    "                                                 mode='max', save_freq='epoch')\n",
    "history_net= model_net.fit(features, labels, epochs=50, batch_size=16, validation_split=0.2, callbacks=[checkpoint_net])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 768)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_net.load_weights('mymodel1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#обработаем тестовые данные\n",
    "tokenized_test = test.text.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "max_len = 0\n",
    "for i in tokenized_test.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded_test = np.array([i + [0]*(max_len-len(i)) for i in tokenized_test.values])\n",
    "\n",
    "input_ids = torch.tensor(np.array(padded_test)).to(torch.int64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids)\n",
    "    \n",
    "features_test = last_hidden_states[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "svc на тесте выдал результат 0,837.                               \n",
    "Попробуем также объединить модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C=1, solver='liblinear')\n",
    "svc = LinearSVC(C=2, loss='hinge',random_state = 42)\n",
    "sgd = SGDClassifier(alpha = 0.0001, loss = 'modified_huber', n_jobs=-1, penalty='l1')\n",
    "xgboost = XGBClassifier(300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=2, loss='hinge', random_state=42)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logit.fit(features, labels)\n",
    "svc.fit(features, labels)\n",
    "# sgd.fit(features, labels)\n",
    "# xgboost.fit(features, labels)\n",
    "# model_net.load_weights('mymodel1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_pred = sgd.predict(features_test)\n",
    "logit_pred = logit.predict(features_test)\n",
    "svc_pred = svc.predict(features_test)\n",
    "net = model_net.predict(features_test)\n",
    "net_pred = [0 if i< 0.5 else 1 for i in net[:,1]]\n",
    "xgb_pred = xgboost.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred = pd.DataFrame()\n",
    "\n",
    "all_pred['sgd'] = sgd_pred\n",
    "all_pred['logit'] = logit_pred\n",
    "all_pred['svc'] = svc_pred\n",
    "all_pred['net'] = net_pred\n",
    "all_pred['xgb'] = xgb_pred\n",
    "\n",
    "all_model = all_pred.sum(axis=1)\n",
    "answer = [0 if i < 3 else 1 for i in all_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "example1 = pd.read_csv(\"products_sentiment_sample_submission.csv\")\n",
    "example1['y'] = answer\n",
    "example1.to_csv('submission_ton.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как итог получаем результат 0,849 и 15 место в соревновании"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
